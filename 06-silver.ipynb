{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c2bbfe-9388-4751-9e7c-e5529a230de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c69fa3a-a146-4954-a47f-2b1429f8a7c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view_name):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name \n",
    "        \n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class Silver():\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config() \n",
    "        self.checkpoint_base = self.Conf.base_dir_checkpoint + \"/checkpoints\"\n",
    "        self.catalog = env\n",
    "        self.db_sv_name = \"ecommerce_db_sv\"\n",
    "        self.db_bz_name = \"ecommerce_db_bz\"\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_sv_name}\")\n",
    "\n",
    "    def cleanup(self): \n",
    "        print(f\"Đang xóa {self.checkpoint_base}...\", end='')\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/customers_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/orders_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/order_items_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/order_payment_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/order_reviews_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/products_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/sellers_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/sale_sv\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/order_detail_sv\", True)\n",
    "        \n",
    "        print(\"Hoàn thành!\")  \n",
    "\n",
    "    def upsert_customer_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.customer_sv target\n",
    "            USING customer_delta source\n",
    "            ON target.customer_id=source.customer_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            target.customer_unique_id = source.customer_unique_id,\n",
    "            target.customer_city = source.customer_city,\n",
    "            target.customer_state = source.customer_state\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            customer_id, customer_unique_id, customer_city, customer_state\n",
    "            ) VALUES (\n",
    "            source.customer_id, source.customer_unique_id, source.customer_city, source.customer_state\n",
    "            )\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"customer_delta\")\n",
    "        df_delta = (spark.readStream\n",
    "                         .table(f\"{self.catalog}.{self.db_bz_name}.customer_bz\")\n",
    "                         .selectExpr(\"customer_id\",\"customer_unique_id\", \"customer_city\", \"customer_state\")\n",
    "                         .dropDuplicates([\"customer_id\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/customers_sv\")\n",
    "                                 .queryName(\"customers_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p1\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_orders_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.orders_sv AS target\n",
    "            USING orders_delta AS source\n",
    "            ON target.order_id = source.order_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            target.order_status = source.order_status,\n",
    "            target.order_approved_at = source.order_approved_at,\n",
    "            target.order_delivered_carrier_date = source.order_delivered_carrier_date,\n",
    "            target.order_delivered_customer_date = source.order_delivered_customer_date,\n",
    "            target.order_estimated_delivery_date = source.order_estimated_delivery_date\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            order_id, customer_id, order_status,\n",
    "            order_purchase_timestamp, order_approved_at,\n",
    "            order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date\n",
    "            ) VALUES (\n",
    "            source.order_id, source.customer_id, source.order_status,\n",
    "            source.order_purchase_timestamp, source.order_approved_at,\n",
    "            source.order_delivered_carrier_date, source.order_delivered_customer_date, source.order_estimated_delivery_date\n",
    "            )\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"orders_delta\")\n",
    "        df_delta = (spark.readStream\n",
    "                         .table(f\"{self.catalog}.{self.db_bz_name}.orders_bz\")\n",
    "                         .selectExpr(\"order_id\", \"customer_id\",\"order_status\", \"order_purchase_timestamp\", \"order_approved_at\", \"order_delivered_carrier_date\", \"order_delivered_customer_date\",\"order_estimated_delivery_date\")\n",
    "                         .dropDuplicates([\"order_id\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/orders_sv\")\n",
    "                                 .queryName(\"orders_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_order_items_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.order_items_sv AS target\n",
    "            USING order_items_delta AS source\n",
    "            ON target.order_id = source.order_id AND target.order_item_id = source.order_item_id AND target.product_id = source.product_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            target.seller_id = source.seller_id,\n",
    "            target.shipping_limit_date = source.shipping_limit_date,\n",
    "            target.price = source.price,\n",
    "            target.freight_value = source.freight_value\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            order_id, order_item_id, product_id, seller_id, shipping_limit_date, price, freight_value\n",
    "            ) VALUES (\n",
    "            source.order_id, source.order_item_id, source.product_id, source.seller_id, source.shipping_limit_date, source.price, source.freight_value\n",
    "            );\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"order_items_delta\")\n",
    "        df_delta = (spark.readStream\n",
    "                         .table(f\"{self.catalog}.{self.db_bz_name}.order_items_bz\")\n",
    "                         .selectExpr(\"order_id\", \"order_item_id\", \"product_id\", \"seller_id\", \"shipping_limit_date\", \"price\", \"freight_value\")\n",
    "                         .dropDuplicates([\"order_id\", \"order_item_id\", \"product_id\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/order_items_sv\")\n",
    "                                 .queryName(\"order_items_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "    def delete_null_category_items(self):\n",
    "        delete_query = f\"\"\"\n",
    "            DELETE FROM {self.catalog}.{self.db_sv_name}.order_items_sv\n",
    "            WHERE product_id IN (\n",
    "                SELECT product_id\n",
    "                FROM {self.catalog}.{self.db_sv_name}.products_sv\n",
    "                WHERE product_category_name IS NULL\n",
    "            )\n",
    "        \"\"\"\n",
    "        spark.sql(delete_query)\n",
    "    \n",
    "    def upsert_order_payment_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.order_payment_sv AS target\n",
    "            USING order_payment_delta AS source\n",
    "            ON target.order_id = source.order_id\n",
    "            AND target.payment_sequential = source.payment_sequential\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            target.payment_type = source.payment_type,\n",
    "            target.payment_installments = source.payment_installments,\n",
    "            target.payment_value = source.payment_value\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            order_id, payment_sequential, payment_type,\n",
    "            payment_installments, payment_value\n",
    "        ) VALUES (\n",
    "            source.order_id, source.payment_sequential, source.payment_type,\n",
    "            source.payment_installments, source.payment_value\n",
    "        )\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"order_payment_delta\")\n",
    "        df_delta = (spark.readStream\n",
    "                         .table(f\"{self.catalog}.{self.db_bz_name}.order_payment_bz\")\n",
    "                         .selectExpr(\"order_id\", \"payment_sequential\", \"payment_type\", \"payment_installments\" , \"payment_value\")\n",
    "                         .dropDuplicates([\"order_id\", \"payment_sequential\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/order_payment_sv\")\n",
    "                                 .queryName(\"order_payment_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p4\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_order_reviews_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.order_reviews_sv AS target\n",
    "            USING order_reviews_delta AS source\n",
    "            ON target.review_id = source.review_id AND target.order_id = source.order_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            target.review_score = source.review_score,\n",
    "            target.review_comment_title = source.review_comment_title,\n",
    "            target.review_comment_message = source.review_comment_message,\n",
    "            target.review_creation_date = source.review_creation_date,\n",
    "            target.review_answer_timestamp = source.review_answer_timestamp\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            review_id, order_id, review_score, review_comment_title,\n",
    "            review_comment_message, review_creation_date, review_answer_timestamp\n",
    "            ) VALUES (\n",
    "            source.review_id, source.order_id, source.review_score, source.review_comment_title,\n",
    "            source.review_comment_message, source.review_creation_date, source.review_answer_timestamp\n",
    "            )\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"order_reviews_delta\")\n",
    "        df_delta = (spark.readStream\n",
    "                         .table(f\"{self.catalog}.{self.db_bz_name}.order_reviews_bz\")\n",
    "                         .selectExpr(\"review_id\" , \"order_id\", \"review_score\", \"review_comment_title\", \"review_comment_message\", \"review_creation_date\",\"review_answer_timestamp\")\n",
    "                         .filter(\"review_score IS NOT NULL\")\n",
    "                         .dropDuplicates([\"review_id\", \"order_id\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/order_reviews_sv\")\n",
    "                                 .queryName(\"order_reviews_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p5\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_sellers_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.sellers_sv AS target\n",
    "            USING sellers_delta AS source\n",
    "            ON target.seller_id = source.seller_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "            target.seller_city = source.seller_city,\n",
    "            target.seller_state = source.seller_state\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "            seller_id, seller_city, seller_state\n",
    "            ) VALUES (\n",
    "            source.seller_id, source.seller_city, source.seller_state\n",
    "            )\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"sellers_delta\")\n",
    "        df_delta = (spark.readStream\n",
    "                         .table(f\"{self.catalog}.{self.db_bz_name}.sellers_bz\")\n",
    "                         .selectExpr(\"seller_id\", \"seller_city\", \"seller_state\")\n",
    "                         .dropDuplicates([\"seller_id\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/sellers_sv\")\n",
    "                                 .queryName(\"sellers_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p6\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_date_lookup_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.date_lookup_sv AS target\n",
    "            USING date_lookup_delta AS source\n",
    "            ON target.DateKey = source.DateKey\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                DateKey, Date, DayOfWeek, DayName, DayOfMonth, DayOfYear, MonthName, MonthOfYear, Quarter, QuarterName, Year, IsWeekday\n",
    "            ) VALUES (\n",
    "                source.DateKey, source.Date, source.DayOfWeek, source.DayName,\n",
    "                source.DayOfMonth, source.DayOfYear, source.MonthName, source.MonthOfYear, source.Quarter,\n",
    "                source.QuarterName, source.Year, source.IsWeekday\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"date_lookup_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            spark.readStream\n",
    "                .table(f\"{self.catalog}.{self.db_bz_name}.date_lookup_bz\")\n",
    "                .selectExpr(\n",
    "                    \"date_key as DateKey\",\n",
    "                    \"full_date as Date\",\n",
    "                    \"day_of_week as DayOfWeek\",\n",
    "                    \"day_name as DayName\",\n",
    "                    \"day_num_in_month as DayOfMonth\",\n",
    "                    \"day_num_overall as DayOfYear\",\n",
    "                    \"month_name as MonthName\",\n",
    "                    \"month as MonthOfYear\",\n",
    "                    \"quarter as Quarter\",\n",
    "                    \"\"\"CASE \n",
    "                        WHEN month >= 1 AND month <= 3 THEN 'First'\n",
    "                        WHEN month >= 4 AND month <= 6 THEN 'Second'\n",
    "                        WHEN month >= 7 AND month <= 9 THEN 'Third'\n",
    "                        WHEN month >= 10 AND month <= 12 THEN 'Fourth'\n",
    "                    END AS QuarterName\"\"\",\n",
    "                    \"year as Year\",\n",
    "                    \"weekday_flag as IsWeekday\"\n",
    "                )\n",
    "                .dropDuplicates([\"DateKey\"])\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/date_lookup_sv\")\n",
    "                .queryName(\"date_lookup_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p7\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def insert_special_date_lookup_sv(self):\n",
    "        from delta.tables import DeltaTable\n",
    "\n",
    "        table_name = f\"{self.catalog}.{self.db_sv_name}.date_lookup_sv\"\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "        df = delta_table.toDF().filter(\"DateKey = -1\")\n",
    "        if df.count() == 0:\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO {table_name}\n",
    "                VALUES (-1, NULL, 0, '', 0, 0, '', 0, 0, '', 0, '0')\n",
    "            \"\"\")\n",
    "            print(\"Đã điền DateKey = -1 record.\")\n",
    "        else:\n",
    "            print(\"DateKey = -1 record dã tồn tại.\")\n",
    "\n",
    "    def upsert_products_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.products_sv AS target\n",
    "            USING products_delta AS source\n",
    "            ON target.product_id = source.product_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.product_category_name = source.product_category_name_english,\n",
    "                target.product_name_lenght = source.product_name_lenght,\n",
    "                target.product_description_lenght = source.product_description_lenght,\n",
    "                target.product_photos_qty = source.product_photos_qty,\n",
    "                target.product_weight_g = source.product_weight_g,\n",
    "                target.product_length_cm = source.product_length_cm,\n",
    "                target.product_height_cm = source.product_height_cm,\n",
    "                target.product_width_cm = source.product_width_cm\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                product_id,\n",
    "                product_category_name,\n",
    "                product_name_lenght,\n",
    "                product_description_lenght,\n",
    "                product_photos_qty,\n",
    "                product_weight_g,\n",
    "                product_length_cm,\n",
    "                product_height_cm,\n",
    "                product_width_cm\n",
    "            ) VALUES (\n",
    "                source.product_id,\n",
    "                source.product_category_name_english,\n",
    "                source.product_name_lenght,\n",
    "                source.product_description_lenght,\n",
    "                source.product_photos_qty,\n",
    "                source.product_weight_g,\n",
    "                source.product_length_cm,\n",
    "                source.product_height_cm,\n",
    "                source.product_width_cm\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"products_delta\")\n",
    "\n",
    "        df_products = (\n",
    "            spark.readStream\n",
    "                .table(f\"{self.catalog}.{self.db_bz_name}.products_bz\")\n",
    "                .filter(F.col(\"product_category_name\").isNotNull())\n",
    "        )\n",
    "\n",
    "        df_category = spark.table(f\"{self.catalog}.{self.db_bz_name}.category_translation_bz\")\n",
    "\n",
    "        df_delta = (\n",
    "            df_products.alias(\"p\")\n",
    "                .join(\n",
    "                    df_category.alias(\"c\"),\n",
    "                    F.col(\"p.product_category_name\") == F.col(\"c.product_category_name\"),\n",
    "                    how=\"inner\"\n",
    "                )\n",
    "                .select(\n",
    "                    F.col(\"p.product_id\"),\n",
    "                    F.col(\"c.product_category_name_english\"),\n",
    "                    F.col(\"p.product_name_lenght\"),\n",
    "                    F.col(\"p.product_description_lenght\"),\n",
    "                    F.col(\"p.product_photos_qty\"),\n",
    "                    F.col(\"p.product_weight_g\"),\n",
    "                    F.col(\"p.product_length_cm\"),\n",
    "                    F.col(\"p.product_height_cm\"),\n",
    "                    F.col(\"p.product_width_cm\")\n",
    "                )\n",
    "                .dropDuplicates([\"product_id\"])\n",
    "            )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/products_sv\")\n",
    "                .queryName(\"products_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p8\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_sale_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.sale_sv target\n",
    "            USING temp_sale_delta source\n",
    "            ON target.order_id = source.order_id \n",
    "            AND target.order_item_id = source.order_item_id\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"temp_sale_delta\")\n",
    "\n",
    "        df_order_items = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.order_items_bz\")\n",
    "        df_orders = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.orders_bz\")\n",
    "        df_customers = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.customer_bz\")\n",
    "        df_products = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.products_bz\")\n",
    "        df_sellers = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.sellers_bz\")\n",
    "\n",
    "        df_delta = (\n",
    "            df_order_items.alias(\"oi\")\n",
    "            .join(df_orders.alias(\"o\"), F.col(\"oi.order_id\") == F.col(\"o.order_id\"))\n",
    "            .join(df_sellers.alias(\"s\"), F.col(\"oi.seller_id\") == F.col(\"s.seller_id\"))\n",
    "            .join(df_products.alias(\"p\"), F.col(\"oi.product_id\") == F.col(\"p.product_id\"))\n",
    "            .join(df_customers.alias(\"c\"), F.col(\"o.customer_id\") == F.col(\"c.customer_id\"))\n",
    "            .select(\n",
    "                \"oi.order_id\",\n",
    "                \"c.customer_id\",\n",
    "                \"p.product_id\",\n",
    "                \"s.seller_id\",\n",
    "                \"oi.order_item_id\",\n",
    "                \"oi.price\",\n",
    "                \"oi.freight_value\",\n",
    "                \"o.order_purchase_timestamp\",\n",
    "                \"o.order_delivered_customer_date\",\n",
    "                \"o.order_estimated_delivery_date\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"append\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/sale_sv\")\n",
    "                .queryName(\"sale_sv_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p9\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "    \n",
    "    def upsert_order_detail_sv(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_sv_name}.order_detail_sv target\n",
    "            USING temp_order_detail_delta source\n",
    "            ON target.order_id = source.order_id\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"temp_order_detail_delta\")\n",
    "\n",
    "        df_order_items = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.order_items_bz\")\n",
    "        df_orders = spark.readStream.table(f\"{self.catalog}.{self.db_bz_name}.orders_bz\")\n",
    "\n",
    "        df_delta = (\n",
    "            df_order_items.alias(\"oi\")\n",
    "            .join(df_orders.alias(\"o\"), F.col(\"oi.order_id\") == F.col(\"o.order_id\"))\n",
    "            .select(\n",
    "                \"oi.order_id\",  \n",
    "                \"o.customer_id\",\n",
    "                \"o.order_purchase_timestamp\",\n",
    "                \"o.order_delivered_customer_date\",\n",
    "                \"o.order_estimated_delivery_date\",\n",
    "                \"oi.price\",\n",
    "                \"oi.freight_value\",\n",
    "                \"o.order_approved_at\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"append\") \n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/order_detail_sv\")\n",
    "                .queryName(\"order_detail_sv_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p10\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "    def load(self, once=True, processing_time=\"15 seconds\"):\n",
    "        print(f\"\\nTiến hành đưa dữ liệu vào Silver Layer...\")\n",
    "        self.upsert_customer_sv(once, processing_time) \n",
    "        self.upsert_orders_sv(once, processing_time)\n",
    "        self.upsert_order_payment_sv(once, processing_time) \n",
    "        self.upsert_order_reviews_sv(once, processing_time) \n",
    "        self.upsert_products_sv(once, processing_time)\n",
    "        self.upsert_order_items_sv(once, processing_time) \n",
    "        self.insert_special_date_lookup_sv()\n",
    "        self.upsert_date_lookup_sv(once, processing_time)\n",
    "        self.upsert_sellers_sv(once, processing_time)\n",
    "        self.delete_null_category_items()\n",
    "        self.upsert_sale_sv(once, processing_time)\n",
    "        self.upsert_order_detail_sv(once, processing_time)\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "\n",
    "        print(f\"Hoàn thành đưa dữ liệu vào Silver Layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cfb20c-0cf0-487c-ace0-457e65ef5dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SV=Silver(\"dev\")\n",
    "SV.load(once=True) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
