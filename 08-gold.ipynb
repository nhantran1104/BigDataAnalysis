{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0441e605-4e63-4b33-9ee6-8f7b64a97a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe49dd6-1c42-4138-9ef7-c32c9d94955d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view_name):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name \n",
    "        \n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class Gold():\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config() \n",
    "        self.checkpoint_base = self.Conf.base_dir_checkpoint + \"/checkpoints\"\n",
    "        self.catalog = env\n",
    "        self.db_sv_name = \"ecommerce_db_sv\"\n",
    "        self.db_gd_name = \"ecommerce_db_gd\"\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_gd_name}\")\n",
    "\n",
    "    def cleanup(self): \n",
    "        print(f\"Đang xóa {self.checkpoint_base}...\", end='')\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/dim_customer_gd\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/dim_product_gd\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/dim_seller_gd\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/dim_time_gd\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/fact_order_gd\", True)\n",
    "        dbutils.fs.rm(f\"{self.checkpoint_base}/fact_sale_gd\", True)\n",
    "\n",
    "        print(\"Hoàn thành!\")  \n",
    "\n",
    "    def upsert_dim_customer_gd(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_gd_name}.dim_customer_gd target\n",
    "            USING customer_delta source\n",
    "            ON target.CustomerID = source.customer_id\n",
    "\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.CustomerCity = source.customer_city,\n",
    "                target.CustomerState = source.customer_state\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (CustomerID, CustomerCity, CustomerState)\n",
    "            VALUES (source.customer_id, source.customer_city, source.customer_state)\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"customer_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            spark.readStream\n",
    "                .table(f\"{self.catalog}.{self.db_sv_name}.customer_sv\")\n",
    "                .selectExpr(\"customer_id\", \"customer_unique_id\", \"customer_city\", \"customer_state\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/dim_customer_gd\")\n",
    "                .queryName(\"customers_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"gold_p1\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_dim_product_gd(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_gd_name}.dim_product_gd target\n",
    "            USING product_delta source\n",
    "            ON target.ProductID = source.product_id\n",
    "\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.CategoryName = source.category_name\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (ProductID, CategoryName)\n",
    "            VALUES (source.product_id, source.category_name)\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"product_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            spark.readStream\n",
    "                .table(f\"{self.catalog}.{self.db_sv_name}.products_sv\")\n",
    "                .selectExpr(\"product_id\", \"product_category_name as category_name\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/dim_product_gd\")\n",
    "                .queryName(\"dim_product_gd_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"gold_p2\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_dim_seller_gd(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_gd_name}.dim_seller_gd target\n",
    "            USING seller_delta source\n",
    "            ON target.SellerID = source.seller_id\n",
    "\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.SellerCity = source.seller_city,\n",
    "                target.SellerState = source.seller_state\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (SellerID, SellerCity, SellerState)\n",
    "            VALUES (source.seller_id, source.seller_city, source.seller_state)\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"seller_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            spark.readStream\n",
    "                .table(f\"{self.catalog}.{self.db_sv_name}.sellers_sv\")\n",
    "                .selectExpr(\"seller_id\", \"seller_city\", \"seller_state\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/dim_seller_gd\")\n",
    "                .queryName(\"dim_seller_gd_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"gold_p3\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_dim_time_gd(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_gd_name}.dim_time_gd target\n",
    "            USING time_delta source\n",
    "            ON target.DateKey = source.DateKey\n",
    "\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.Date = source.Date,\n",
    "                target.DayOfWeek = source.DayOfWeek,\n",
    "                target.DayName = source.DayName,\n",
    "                target.DayOfMonth = source.DayOfMonth,\n",
    "                target.DayOfYear = source.DayOfYear,\n",
    "                target.MonthName = source.MonthName,\n",
    "                target.MonthOfYear = source.MonthOfYear,\n",
    "                target.Quarter = source.Quarter,\n",
    "                target.QuarterName = source.QuarterName,\n",
    "                target.Year = source.Year,\n",
    "                target.IsWeekday = source.IsWeekday\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                DateKey, Date, DayOfWeek, DayName, DayOfMonth, DayOfYear,\n",
    "                MonthName, MonthOfYear, Quarter, QuarterName, Year, IsWeekday\n",
    "            )\n",
    "            VALUES (\n",
    "                source.DateKey, source.Date, source.DayOfWeek, source.DayName, source.DayOfMonth,\n",
    "                source.DayOfYear, source.MonthName, source.MonthOfYear,\n",
    "                source.Quarter, source.QuarterName, source.Year, source.IsWeekday\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"time_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            spark.readStream\n",
    "                .table(f\"{self.catalog}.{self.db_sv_name}.date_lookup_sv\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/dim_time_gd\")\n",
    "                .queryName(\"dim_time_gd_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"gold_p4\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_fact_sale_gd(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_gd_name}.fact_sale_gd AS target\n",
    "            USING fact_sale_delta AS source\n",
    "            ON target.order_item_id = source.order_item_id\n",
    "            AND target.order_id = source.order_id\n",
    "            AND target.product_id = source.product_id\n",
    "\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.PurchaseDateKey = source.PurchaseDateKey,\n",
    "                target.DeliveredDateKey = source.DeliveredDateKey,\n",
    "                target.EstimateDeliveredDateKey = source.EstimateDeliveredDateKey,\n",
    "                target.CustomerKey = source.CustomerKey,\n",
    "                target.ProductKey = source.ProductKey,\n",
    "                target.SellerKey = source.SellerKey,\n",
    "                target.Price = source.Price,\n",
    "                target.FreightValue = source.FreightValue,\n",
    "                target.TotalValue = source.TotalValue\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                PurchaseDateKey, DeliveredDateKey, EstimateDeliveredDateKey,\n",
    "                CustomerKey, ProductKey, SellerKey,\n",
    "                order_item_id, order_id, product_id,\n",
    "                Price, FreightValue, TotalValue\n",
    "            )\n",
    "            VALUES (\n",
    "                source.PurchaseDateKey, source.DeliveredDateKey, source.EstimateDeliveredDateKey,\n",
    "                source.CustomerKey, source.ProductKey, source.SellerKey,\n",
    "                source.order_item_id, source.order_id, source.product_id,\n",
    "                source.Price, source.FreightValue, source.TotalValue\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"fact_sale_delta\")\n",
    "\n",
    "        sale_sv = spark.readStream.table(f\"{self.catalog}.{self.db_sv_name}.sale_sv\").alias(\"sale_sv\")\n",
    "        dim_customer = spark.table(f\"{self.catalog}.{self.db_gd_name}.dim_customer_gd\").alias(\"dim_customer\")\n",
    "        dim_seller = spark.table(f\"{self.catalog}.{self.db_gd_name}.dim_seller_gd\").alias(\"dim_seller\")\n",
    "        dim_product = spark.table(f\"{self.catalog}.{self.db_gd_name}.dim_product_gd\").alias(\"dim_product\")\n",
    "\n",
    "        df_delta = (\n",
    "            sale_sv\n",
    "            .join(dim_customer, F.col(\"sale_sv.customer_id\") == F.col(\"dim_customer.CustomerID\"))\n",
    "            .join(dim_seller, F.col(\"sale_sv.seller_id\") == F.col(\"dim_seller.SellerID\"))\n",
    "            .join(dim_product, F.col(\"sale_sv.product_id\") == F.col(\"dim_product.ProductID\"))\n",
    "            .selectExpr(\n",
    "                \"cast(date_format(order_purchase_timestamp, 'yyyyMMdd') as int) as PurchaseDateKey\",\n",
    "                \"case when order_delivered_customer_date is null then -1 else cast(date_format(order_delivered_customer_date, 'yyyyMMdd') as int) end as DeliveredDateKey\",\n",
    "                \"cast(date_format(order_estimated_delivery_date, 'yyyyMMdd') as int) as EstimateDeliveredDateKey\",\n",
    "\n",
    "                \"dim_customer.CustomerKey as CustomerKey\",\n",
    "                \"dim_product.ProductKey as ProductKey\",\n",
    "                \"dim_seller.SellerKey as SellerKey\",\n",
    "\n",
    "                \"sale_sv.order_item_id\",\n",
    "                \"sale_sv.order_id\",\n",
    "                \"sale_sv.product_id\",\n",
    "\n",
    "                \"sale_sv.price as Price\",\n",
    "                \"sale_sv.freight_value as FreightValue\",\n",
    "                \"(sale_sv.price + sale_sv.freight_value) as TotalValue\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/fact_sale_gd\")\n",
    "                .queryName(\"fact_sale_gd_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"gold_p3\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_fact_order_gd(self, once=True, processing_time=\"15 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_gd_name}.fact_order_gd AS target\n",
    "            USING fact_order_delta AS source\n",
    "            ON target.order_id = source.order_id\n",
    "\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.PurchaseDateKey      = source.PurchaseDateKey,\n",
    "                target.DeliveredDateKey     = source.DeliveredDateKey,\n",
    "                target.EstimateDeliveredDateKey = source.EstimateDeliveredDateKey,\n",
    "                target.CustomerKey          = source.CustomerKey,\n",
    "                target.ShipAmount           = source.ShipAmount,\n",
    "                target.TotalProductValue    = source.TotalProductValue,\n",
    "                target.TotalAmount          = source.TotalAmount,\n",
    "                target.DeliveryActualDays   = source.DeliveryActualDays,\n",
    "                target.DeliveryEstimateDays = source.DeliveryEstimateDays,\n",
    "                target.ApproveDays          = source.ApproveDays\n",
    "\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                PurchaseDateKey, DeliveredDateKey, EstimateDeliveredDateKey,\n",
    "                CustomerKey, order_id,\n",
    "                ShipAmount, TotalProductValue, TotalAmount,\n",
    "                DeliveryActualDays, DeliveryEstimateDays, ApproveDays\n",
    "            ) VALUES (\n",
    "                source.PurchaseDateKey, source.DeliveredDateKey, source.EstimateDeliveredDateKey,\n",
    "                source.CustomerKey, source.order_id,\n",
    "                source.ShipAmount, source.TotalProductValue, source.TotalAmount,\n",
    "                source.DeliveryActualDays, source.DeliveryEstimateDays, source.ApproveDays\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"fact_order_delta\")\n",
    "\n",
    "        order_detail = spark.readStream.table(f\"{self.catalog}.{self.db_sv_name}.order_detail_sv\").alias(\"o\")\n",
    "        dim_customer = spark.table(f\"{self.catalog}.{self.db_gd_name}.dim_customer_gd\").alias(\"c\")\n",
    "\n",
    "        df_delta = (\n",
    "            order_detail\n",
    "            .join(dim_customer, F.col(\"o.customer_id\") == F.col(\"c.CustomerID\"))\n",
    "            .groupBy(\n",
    "                \"o.order_purchase_timestamp\",\n",
    "                \"o.order_delivered_customer_date\",\n",
    "                \"o.order_estimated_delivery_date\",\n",
    "                \"c.CustomerKey\",\n",
    "                \"o.order_id\",\n",
    "                \"o.order_approved_at\"\n",
    "            )\n",
    "            .agg(\n",
    "                F.sum(\"freight_value\").alias(\"ShipAmount\"),\n",
    "                F.sum(\"price\").alias(\"TotalProductValue\")\n",
    "            )\n",
    "            .withColumn(\"TotalAmount\", F.col(\"ShipAmount\") + F.col(\"TotalProductValue\"))\n",
    "            .withColumn(\"PurchaseDateKey\",\n",
    "                (F.dayofmonth(\"order_purchase_timestamp\")\n",
    "                + F.month(\"order_purchase_timestamp\") * 100\n",
    "                + F.year(\"order_purchase_timestamp\") * 10000)\n",
    "            )\n",
    "            .withColumn(\"DeliveredDateKey\",\n",
    "                F.when(F.col(\"order_delivered_customer_date\").isNull(), F.lit(-1))\n",
    "                .otherwise(\n",
    "                    F.dayofmonth(\"order_delivered_customer_date\")\n",
    "                    + F.month(\"order_delivered_customer_date\") * 100\n",
    "                    + F.year(\"order_delivered_customer_date\") * 10000\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\"EstimateDeliveredDateKey\",\n",
    "                F.dayofmonth(\"order_estimated_delivery_date\")\n",
    "                + F.month(\"order_estimated_delivery_date\") * 100\n",
    "                + F.year(\"order_estimated_delivery_date\") * 10000\n",
    "            )\n",
    "            .withColumn(\"DeliveryActualDays\",\n",
    "                F.datediff(\"order_delivered_customer_date\", \"order_approved_at\")\n",
    "            )\n",
    "            .withColumn(\"DeliveryEstimateDays\",\n",
    "                F.datediff(\"order_estimated_delivery_date\", \"order_approved_at\")\n",
    "            )\n",
    "            .withColumn(\"ApproveDays\",\n",
    "                F.datediff(\"order_approved_at\", \"order_purchase_timestamp\")\n",
    "            )\n",
    "            .selectExpr(\n",
    "                \"PurchaseDateKey\",\n",
    "                \"DeliveredDateKey\",\n",
    "                \"EstimateDeliveredDateKey\",\n",
    "                \"CustomerKey\",\n",
    "                \"order_id\",\n",
    "                \"ShipAmount\",\n",
    "                \"TotalProductValue\",\n",
    "                \"TotalAmount\",\n",
    "                \"DeliveryActualDays\",\n",
    "                \"DeliveryEstimateDays\",\n",
    "                \"ApproveDays\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "                .foreachBatch(data_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/fact_order_gd\")\n",
    "                .queryName(\"fact_order_gd_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"gold_p4\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def load_dim(self, once=True, processing_time=\"15 seconds\"):\n",
    "        print(\"Tiến hành load dữ liệu vào Gold Layer...\")  \n",
    "        self.upsert_dim_customer_gd(once, processing_time)\n",
    "        self.upsert_dim_product_gd(once, processing_time)\n",
    "        self.upsert_dim_seller_gd(once, processing_time)\n",
    "        self.upsert_dim_time_gd(once, processing_time)\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "\n",
    "    def load_fact(self, once=True, processing_time=\"15 seconds\"):\n",
    "        print(\"Tiến hành load dữ liệu vào Gold Layer...\")  \n",
    "        self.upsert_fact_order_gd(once, processing_time)\n",
    "        self.upsert_fact_sale_gd(once, processing_time)\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667b903b-efb4-417c-99da-7f98f23f150c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gd = Gold(\"dev\")\n",
    "gd.cleanup()\n",
    "gd.load_dim(once=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b455087-9a3e-48b8-8ec1-c32682730e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gd.load_fact(once=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
