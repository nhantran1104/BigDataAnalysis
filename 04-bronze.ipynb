{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6434cb-756e-468a-91d4-8b4d24778797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7754e217-8eb0-43ba-a1c8-25171d76c6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config()\n",
    "        self.landing_zone = self.Conf.base_data_path + \"/raw\" \n",
    "        self.checkpoint_base = self.Conf.base_dir_checkpoint + \"/checkpoints\"\n",
    "        self.catalog = env\n",
    "        self.db_name = \"ecommerce_db_bz\"\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "# --------------------------------INSERT DATA--------------------------------\n",
    "    def insert_order_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"order_id STRING, customer_id STRING, order_status STRING, order_purchase_timestamp TIMESTAMP, order_approved_at TIMESTAMP, order_delivered_carrier_date TIMESTAMP, order_delivered_customer_date TIMESTAMP, order_estimated_delivery_date TIMESTAMP\"\n",
    "\n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/orders\")\n",
    "                )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                 .format(\"delta\") \\\n",
    "                                 .option(\"checkpointLocation\", self.checkpoint_base + \"/order_bz\") \\\n",
    "                                 .outputMode(\"append\") \\\n",
    "                                 .queryName(\"order_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p1\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.orders_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.orders_bz\")\n",
    "        \n",
    "        print(\"Hoàn thành!\")\n",
    "\n",
    "    def insert_order_item_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"order_id STRING, order_item_id INT, product_id STRING, seller_id STRING, shipping_limit_date TIMESTAMP, price FLOAT, freight_value FLOAT\"\n",
    "\n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/order_items\")\n",
    "                )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                 .format(\"delta\") \\\n",
    "                                 .option(\"checkpointLocation\", self.checkpoint_base + \"/order_items_bz\") \\\n",
    "                                 .outputMode(\"append\") \\\n",
    "                                 .queryName(\"order_items_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p2\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.order_items_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.order_items_bz\")\n",
    "        \n",
    "        print(\"Hoàn thành!\")\n",
    "    \n",
    "    def load_customer_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"\"\"\n",
    "            customer_id string,\n",
    "            customer_unique_id string,\n",
    "            customer_zip_code_prefix int,\n",
    "            customer_city string,\n",
    "            customer_state string\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/customers\")\n",
    "                )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                .format(\"delta\") \\\n",
    "                                .option(\"checkpointLocation\", self.checkpoint_base + \"/customer_bz\") \\\n",
    "                                .outputMode(\"append\") \\\n",
    "                                .queryName(\"customer_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p3\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.customer_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.customer_bz\")\n",
    "        print(\"Hoàn thành!\")\n",
    "    \n",
    "    def insert_category_translation_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"\"\"\n",
    "            product_category_name STRING, product_category_name_english STRING\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/product_category_name_translation\")\n",
    "                )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                .format(\"delta\") \\\n",
    "                                .option(\"checkpointLocation\", self.checkpoint_base + \"/category_translation_bz\") \\\n",
    "                                .outputMode(\"append\") \\\n",
    "                                .queryName(\"category_translation_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p4\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.category_translation_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.category_translation_bz\")\n",
    "        print(\"Hoàn thành!\")\n",
    "\n",
    "    def load_sellers_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"\"\"\n",
    "            seller_id string,\n",
    "            seller_zip_code_prefix int,\n",
    "            seller_city string,\n",
    "            seller_state string\n",
    "        \"\"\"\n",
    "        df = (spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .schema(schema)\n",
    "                    .option(\"maxFilesPerTrigger\", 1)\n",
    "                    .option(\"cloudFiles.format\", \"csv\")\n",
    "                    .option(\"header\", \"true\")\n",
    "                    .load(self.landing_zone + \"/sellers\")\n",
    "                    )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                .format(\"delta\") \\\n",
    "                                .option(\"checkpointLocation\", self.checkpoint_base + \"/sellers_bz\") \\\n",
    "                                .outputMode(\"append\") \\\n",
    "                                .queryName(\"sellers_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p5\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.sellers_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.sellers_bz\")\n",
    "        print(\"Hoàn thành!\")\n",
    "\n",
    "    def load_order_payments_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"\"\"\n",
    "            order_id string,\n",
    "            payment_sequential tinyint,\n",
    "            payment_type string,\n",
    "            payment_installments tinyint,\n",
    "            payment_value float\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/order_payments\")\n",
    "                )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                .format(\"delta\") \\\n",
    "                                .option(\"checkpointLocation\", self.checkpoint_base + \"/order_payments_bz\") \\\n",
    "                                .outputMode(\"append\") \\\n",
    "                                .queryName(\"order_payments_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p5\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.order_payment_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.order_payment_bz\")\n",
    "        print(\"Hoàn thành!\")\n",
    "\n",
    "    def load_order_reviews_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"\"\"\n",
    "            review_id string,\n",
    "            order_id string,\n",
    "            review_score tinyint,\n",
    "            review_comment_title string,\n",
    "            review_comment_message string,\n",
    "            review_creation_date timestamp,\n",
    "            review_answer_timestamp timestamp\n",
    "        \"\"\"\n",
    "\n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/order_reviews\")\n",
    "                )\n",
    "\n",
    "        stream_writer = df.writeStream \\\n",
    "                                .format(\"delta\") \\\n",
    "                                .option(\"checkpointLocation\", self.checkpoint_base + \"/order_reviews_bz\") \\\n",
    "                                .outputMode(\"append\") \\\n",
    "                                .queryName(\"order_reviews_bz_insert_stream\")\n",
    "\n",
    "        # spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p7\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.order_reviews_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.order_reviews_bz\")\n",
    "        print(\"Hoàn thành!\")\n",
    "\n",
    "    def insert_product_bz(self, once=True, processing_time=\"5 seconds\"):\n",
    "        from pyspark.sql import functions as F\n",
    "        schema = \"\"\"\n",
    "            product_id STRING, product_category_name STRING, product_name_lenght TINYINT, product_description_lenght INT, product_photos_qty TINYINT, product_weight_g INT, product_length_cm TINYINT, product_height_cm TINYINT, product_width_cm TINYINT\n",
    "        \"\"\"\n",
    "        \n",
    "        df = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .schema(schema)\n",
    "                .option(\"maxFilesPerTrigger\", 1)\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(self.landing_zone + \"/products\")\n",
    "                )\n",
    "    \n",
    "        stream_writer = df.writeStream \\\n",
    "                                .format(\"delta\") \\\n",
    "                                .option(\"checkpointLocation\", self.checkpoint_base + \"/product_bz\") \\\n",
    "                                .outputMode(\"append\") \\\n",
    "                                .queryName(\"product_bz_insert_stream\")\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p8\")\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.products_bz\")\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).toTable(f\"{self.catalog}.{self.db_name}.products_bz\")\n",
    "        print(\"Hoàn thành!\")\n",
    "    \n",
    "\n",
    "    def load(self, once=True, processing_time=\"5 seconds\"):\n",
    "        print(f\"\\nTiến hành đưa dữ liệu vào Bronze Layer...\")\n",
    "        self.insert_order_bz(once, processing_time) \n",
    "        self.insert_order_item_bz(once, processing_time) \n",
    "        self.load_customer_bz(once, processing_time)\n",
    "        self.insert_category_translation_bz(once, processing_time) \n",
    "        self.load_sellers_bz(once, processing_time) \n",
    "        self.load_order_payments_bz(once, processing_time)\n",
    "        self.insert_product_bz(once, processing_time) \n",
    "        self.load_order_reviews_bz(once, processing_time)\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "\n",
    "        print(f\"Hoàn thành đưa dữ liệu vào Bronze Layer\")\n",
    "\n",
    "    def cleanup(self): \n",
    "        print(f\"Đang xóa {self.checkpoint_base}...\", end='')\n",
    "        dbutils.fs.rm(self.checkpoint_base, True)\n",
    "        print(\"Hoàn thành!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc725808-d90d-4c46-b13f-34d74e5f0709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BZ = Bronze('dev')\n",
    "BZ.load(once=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5099698041544251,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04-bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
